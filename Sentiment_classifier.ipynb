{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"data/train.csv\")\n",
    "evaluation=pd.read_csv(\"data/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment classifier v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_imdb_data(data_dir):\n",
    "    \"\"\"Loads the IMDB train/test datasets from a folder path.\n",
    "    Input:\n",
    "    data_dir: path to the \"aclImdb\" folder.\n",
    "    \n",
    "    Returns:\n",
    "    train/test datasets as pandas dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        data[split] = []\n",
    "        for sentiment in [\"neg\", \"pos\"]:\n",
    "            score = 1 if sentiment == \"pos\" else 0\n",
    "\n",
    "            path = os.path.join(data_dir, split, sentiment)\n",
    "            file_names = os.listdir(path)\n",
    "            for f_name in file_names:\n",
    "                with open(os.path.join(path, f_name), \"r\", encoding=\"utf-8\") as f:\n",
    "                    review = f.read()\n",
    "                    data[split].append([review, score])\n",
    "\n",
    "    np.random.shuffle(data[\"train\"])        \n",
    "    data[\"train\"] = pd.DataFrame(data[\"train\"],\n",
    "                                 columns=['text', 'sentiment'])\n",
    "\n",
    "    np.random.shuffle(data[\"test\"])\n",
    "    data[\"test\"] = pd.DataFrame(data[\"test\"],\n",
    "                                columns=['text', 'sentiment'])\n",
    "\n",
    "    return data[\"train\"], data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_train_test_imdb_data(data_dir=\"aclImdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000 tweets.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "twitter_data = pd.read_csv(\"sentiment_data_set/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", header=None)\n",
    "twitter_data.columns = [\"sentiment\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "nb_tweets = len(twitter_data[\"text\"])\n",
    "print(f'{nb_tweets} tweets.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 train tweets.\n",
      "500000 test tweets.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "indices = random.sample(range(nb_tweets), 1000000)\n",
    "\n",
    "twitter_train_data = twitter_data.iloc[indices[:500000]]\n",
    "nb_train_tweets = len(twitter_train_data[\"text\"])\n",
    "print(f'{nb_train_tweets} train tweets.')\n",
    "\n",
    "twitter_test_data = twitter_data.iloc[indices[500000:]]\n",
    "nb_test_tweets = len(twitter_test_data[\"text\"])\n",
    "print(f'{nb_test_tweets} test tweets.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    \n",
    "    while \"http\" in text :\n",
    "        start_url = 0\n",
    "        while (start_url<len(text)-3 and text[start_url:start_url+4] != \"http\") :\n",
    "            start_url += 1\n",
    "        end_url = start_url + 4\n",
    "        while (end_url<len(text) and text[end_url] != \" \") :\n",
    "            end_url += 1\n",
    "            \n",
    "        text=text[:start_url]+text[end_url+1:]\n",
    "        \n",
    "    return text\n",
    "    \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    #remove urls\n",
    "    text = remove_url(text)\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the twitter dataset: 75.72\n"
     ]
    }
   ],
   "source": [
    "# Transform each text into a vector of word counts\n",
    "vectorizer = CountVectorizer(stop_words=\"english\",preprocessor=clean_text)\n",
    "\n",
    "training_features = vectorizer.fit_transform(twitter_train_data[\"text\"])    \n",
    "test_features = vectorizer.transform(twitter_test_data[\"text\"])\n",
    "\n",
    "# Training\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, twitter_train_data[\"sentiment\"])\n",
    "y_pred = model.predict(test_features)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(twitter_test_data[\"sentiment\"], y_pred)\n",
    "\n",
    "print(\"Accuracy on the twitter dataset: {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1588696955143</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>68460</td>\n",
       "      <td>1101</td>\n",
       "      <td>1226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Smh I give up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1588464948124</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>309</td>\n",
       "      <td>51</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Most of us are Human Beings, but I think you m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1588634673360</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3241</td>\n",
       "      <td>1675</td>\n",
       "      <td>2325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Old dirty tricks Trump, at it again...like we ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1588433158672</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32327</td>\n",
       "      <td>667</td>\n",
       "      <td>304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seriously..... I worked 86 hours my last check...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1588582751599</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>581</td>\n",
       "      <td>42</td>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May ALMIGHTY ALLAH have mercy on us all. Only ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665772</th>\n",
       "      <td>665772</td>\n",
       "      <td>1588412684317</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>65355</td>\n",
       "      <td>1984</td>\n",
       "      <td>1902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18 months dawg? Come on man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665773</th>\n",
       "      <td>665773</td>\n",
       "      <td>1588324521711</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1807</td>\n",
       "      <td>2029</td>\n",
       "      <td>347</td>\n",
       "      <td>StanfordEMED</td>\n",
       "      <td>twitter.com/i/web/status/1…</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>Thank you to all of the nurses in our @Stanfor...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665774</th>\n",
       "      <td>665774</td>\n",
       "      <td>1588353174952</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>888</td>\n",
       "      <td>85</td>\n",
       "      <td>257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter.com/i/web/status/1…</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'Post it' pearls for Palliative, End of Life a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665775</th>\n",
       "      <td>665775</td>\n",
       "      <td>1588691378352</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>452</td>\n",
       "      <td>38</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His facial expressions are kind of looking for...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665776</th>\n",
       "      <td>665776</td>\n",
       "      <td>1588432578764</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>590</td>\n",
       "      <td>184</td>\n",
       "      <td>238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We really can't wait.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>665777 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id      timestamp  retweet_count  user_verified  \\\n",
       "0            0  1588696955143              0          False   \n",
       "1            1  1588464948124              0          False   \n",
       "2            2  1588634673360              0          False   \n",
       "3            3  1588433158672              0          False   \n",
       "4            4  1588582751599              0          False   \n",
       "...        ...            ...            ...            ...   \n",
       "665772  665772  1588412684317              0          False   \n",
       "665773  665773  1588324521711              1          False   \n",
       "665774  665774  1588353174952              8          False   \n",
       "665775  665775  1588691378352              0          False   \n",
       "665776  665776  1588432578764              0          False   \n",
       "\n",
       "        user_statuses_count  user_followers_count  user_friends_count  \\\n",
       "0                     68460                  1101                1226   \n",
       "1                       309                    51                 202   \n",
       "2                      3241                  1675                2325   \n",
       "3                     32327                   667                 304   \n",
       "4                       581                    42                 127   \n",
       "...                     ...                   ...                 ...   \n",
       "665772                65355                  1984                1902   \n",
       "665773                 1807                  2029                 347   \n",
       "665774                  888                    85                 257   \n",
       "665775                  452                    38                  91   \n",
       "665776                  590                   184                 238   \n",
       "\n",
       "       user_mentions                         urls hashtags  \\\n",
       "0                NaN                          NaN      NaN   \n",
       "1                NaN                          NaN      NaN   \n",
       "2                NaN                          NaN      NaN   \n",
       "3                NaN                          NaN      NaN   \n",
       "4                NaN                          NaN      NaN   \n",
       "...              ...                          ...      ...   \n",
       "665772           NaN                          NaN      NaN   \n",
       "665773  StanfordEMED  twitter.com/i/web/status/1…  COVID19   \n",
       "665774           NaN  twitter.com/i/web/status/1…      NaN   \n",
       "665775           NaN                          NaN      NaN   \n",
       "665776           NaN                          NaN      NaN   \n",
       "\n",
       "                                                     text  sentiment  \n",
       "0                                           Smh I give up          0  \n",
       "1       Most of us are Human Beings, but I think you m...          0  \n",
       "2       Old dirty tricks Trump, at it again...like we ...          4  \n",
       "3       Seriously..... I worked 86 hours my last check...          0  \n",
       "4       May ALMIGHTY ALLAH have mercy on us all. Only ...          4  \n",
       "...                                                   ...        ...  \n",
       "665772                     18 months dawg? Come on man...          0  \n",
       "665773  Thank you to all of the nurses in our @Stanfor...          4  \n",
       "665774  'Post it' pearls for Palliative, End of Life a...          4  \n",
       "665775  His facial expressions are kind of looking for...          4  \n",
       "665776                              We really can't wait.          4  \n",
       "\n",
       "[665777 rows x 12 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_tweet_features = vectorizer.transform(train[\"text\"])\n",
    "covid_tweet_sentiment_pred = model.predict(covid_tweet_features)\n",
    "train[\"sentiment\"] = covid_tweet_sentiment_pred\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment classifier v2 (didn't use it on our covid data for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, stop_words=[\"the\", \"a\", \"to\", \"in\", \"at\", \"that\"]) #we can remove some stopwords\n",
    "LABEL = data.Field(sequential=False, unk_token=None) #labels are textual so we need to process them too\n",
    "train1, val, test1 = datasets.SST.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [07:42, 1.86MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:24<00:00, 16285.04it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train1, max_size=10000,min_freq=5, vectors=GloVe(name='6B', dim=100)) #convert words to indices, attach vectors to indices\n",
    "LABEL.build_vocab(train1) #need to convert the textual labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #if you have a GPU with CUDA installed, this may speed up computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock', 'is', 'destined', 'be', '21st', 'century', \"'s\", 'new', '``', 'conan', \"''\", 'and', 'he', \"'s\", 'going', 'make', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'] positive\n",
      "len(TEXT.vocab) 3422\n",
      "Most frequent terms [('.', 8024), (',', 7131), ('and', 4473), ('of', 4396), ('is', 2561), (\"'s\", 2544), ('it', 2422), ('as', 1296), ('but', 1172), ('film', 1162)]\n",
      "Conversion of labels to numeric: defaultdict(None, {'positive': 0, 'negative': 1, 'neutral': 2})\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits((train1, test1), batch_size=32, device=device)\n",
    "\n",
    "print(train1[0].text, train1[0].label)\n",
    "\n",
    "#print vocabulary information:\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('Most frequent terms', TEXT.vocab.freqs.most_common(10))\n",
    "print('Conversion of labels to numeric:', LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        \"\"\"\n",
    "        vocab_size: (int) size of the vocabulary - required by embeddings\n",
    "        embed_dim: (int) size of embeddings\n",
    "        num_class: (int) number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #enter here your code\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim,embed_dim,num_layers=2, bidirectional=True)\n",
    "        self.fc = nn.Linear(embed_dim*2, num_class)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, text):\n",
    "        r\"\"\"\n",
    "        Arguments:\n",
    "            text: 1-D tensor representing a bag of text tensors\n",
    "        \"\"\"\n",
    "        #ENTER HERE YOUR CODE\n",
    "        text=self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(text)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        \n",
    "        x=self.fc(hidden)\n",
    "        \n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is 0.032 and training accuracy is 4089/8544     47.86\n",
      "training loss is 0.028 and training accuracy is 5077/8544     59.42\n",
      "training loss is 0.025 and training accuracy is 5602/8544     65.57\n",
      "training loss is 0.021 and training accuracy is 6098/8544     71.37\n",
      "training loss is 0.018 and training accuracy is 6514/8544     76.24\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(3422,100,3)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),lr=1e-3)\n",
    "\n",
    "model.train() #set train mode\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0\n",
    "    for batch in train_iter:\n",
    "        text =batch.text\n",
    "        target=batch.label \n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = F.nll_loss(output,target)\n",
    "        running_loss +=loss\n",
    "        preds = output.data.max(dim=1,keepdim=True)[1]\n",
    "        running_acc += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = running_loss/len(train)\n",
    "\n",
    "        accuracy = 100. * running_acc/len(train)\n",
    "    print(f'training loss is {loss:{5}.{2}} and training accuracy is {running_acc}/{len(train)}{accuracy:{10}.{4}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
