{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  \n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = pd.read_csv(\"data/evaluation_process.csv\")\n",
    "data = pd.read_csv(\"data/training_process.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First part : Classification phase\n",
    "\n",
    "### Transform data\n",
    "\n",
    "First, we need to transform the pd object into numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(L) :\n",
    "    # input \"[1,2,565454,1]\"\n",
    "    # ouput [1,2,565454,1]\n",
    "    T = []\n",
    "    for k in L :\n",
    "        if not k in [\" \",\",\",\"[\",\"]\"] :\n",
    "            try :\n",
    "                x = x*10 + int(k)\n",
    "            except :\n",
    "                x = int(k)\n",
    "        else : \n",
    "            try :\n",
    "                if k != ' ' :\n",
    "                    T.append(x)\n",
    "                    x = 0\n",
    "            except : \n",
    "                count = 0\n",
    "    return T\n",
    "\n",
    "def from_pd_to_numpy(database,training=True) :\n",
    "    X = np.zeros((database.shape[0],21))\n",
    "    y = np.zeros(database.shape[0])\n",
    "    for k in range(database.shape[0]) :\n",
    "        current = database.iloc[k]\n",
    "        if training : \n",
    "            y[k] = current.retweet_count\n",
    "        else :\n",
    "            y[k] = current.id\n",
    "        X[k,0] = current.user_verified\n",
    "        X[k,1] = current.user_statuses_count\n",
    "        X[k,2] = current.user_followers_count\n",
    "        X[k,3] = current.user_friends_count\n",
    "        X[k,4] = current.num_hashtag\n",
    "        X[k,5] = current.got_hashtag\n",
    "        X[k,6] = current.num_at\n",
    "        X[k,7] = current.got_at\n",
    "        X[k,8] = current.num_link\n",
    "        X[k,9] = current.got_link\n",
    "        X[k,10] = current.length\n",
    "        X[k,11] = current.contains_rt\n",
    "        X[k,12] = current.weak\n",
    "        X[k,13] = current.strong\n",
    "        X[k,14] = current.is_upper\n",
    "        X[k,15] = current.contains_excl\n",
    "        X[k,16] = current.contains_per\n",
    "        X[k,17] = current.contains_org\n",
    "        X[k,18] = current.contains_gpe\n",
    "        X[k,19] = current.zeros_pic\n",
    "        X[k,20] = current.sentiment\n",
    "        \"\"\"try : # if we read the database, it will be a string\n",
    "            X[k,11:18] = to_list(current.day)\n",
    "            X[k,18:42] = to_list(current.hour)\n",
    "            X[k,42:54] = to_list(current.month)\n",
    "        except :\n",
    "            X[k,11:18] = current.day\n",
    "            X[k,18:42] = current.hour\n",
    "            X[k,42:54] = current.month\"\"\"\n",
    "    \n",
    "    return X,y   \n",
    "\n",
    "X,y = from_pd_to_numpy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "But now, we do not want yet to predict the number of retweets, but rather classify them. Therefore, we will transform *y* to define 6 classes. 0, <10, <100, <1000, <10000 and the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_classify(y) :\n",
    "    z = np.zeros(y.shape)\n",
    "    for k in range(y.shape[0]) :\n",
    "        f = y[k]\n",
    "        if f > 0 :\n",
    "            if f < 10 :\n",
    "                z[k] = 1\n",
    "            elif f < 100 :\n",
    "                z[k] = 2\n",
    "            elif f < 1000 :\n",
    "                z[k] = 3\n",
    "            elif f < 10000 :\n",
    "                z[k] = 4\n",
    "            else :\n",
    "                z[k] = 5\n",
    "    return z\n",
    "\n",
    "classes = to_classify(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset\n",
    "\n",
    "X contains too much data around 0. We want to have as much tweet with class 0 than 5 in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train(X,y,classes):\n",
    "    max_class = np.amax(classes)\n",
    "    count_class = {\n",
    "        '0' : 0,\n",
    "        '1' : 0,\n",
    "        '2' : 0,\n",
    "        '3' : 0,\n",
    "        '4' : 0,\n",
    "        '5' : 0,\n",
    "        '6' : 0\n",
    "    }\n",
    "    count = 0\n",
    "    for k in range(X.shape[0]) :\n",
    "        if classes[k] == max_class :\n",
    "            count +=1\n",
    "    X_train = np.zeros(X.shape)\n",
    "    y_train = np.zeros(y.shape)\n",
    "    classes_train = np.zeros(classes.shape)\n",
    "    indice = 0\n",
    "    count = int(count*0.7) # do not take the whole data\n",
    "    print(count)\n",
    "    for k in range(X.shape[0]) : \n",
    "        current_class = str(int(classes[k]))\n",
    "        if count_class[current_class] < count*(max_class+ 1 -int(current_class))**1.3 :\n",
    "            X_train[indice,:] = X[k,:]\n",
    "            y_train[indice] = y[k]\n",
    "            classes_train[indice] = classes[k]\n",
    "            count_class[current_class]+=1\n",
    "            indice+=1\n",
    "        \n",
    "    \n",
    "    return X_train[:indice,:],y_train[:indice],classes_train[:indice]\n",
    "\n",
    "X_train, y_train, classes_train = to_train(X,y,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same number of tweet per class\n",
    "\n",
    "To be sure we have the exact same number of tweets per class, we used this SMOTE technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE() \n",
    "X_train,classes_train = oversample.fit_resample(X_train,classes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification via random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=400,max_depth=30, random_state=1)\n",
    "# clf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "# clf = KNeighborsClassifier(n_neighbors=3)\n",
    "# clf = svm.SVC()\n",
    "\n",
    "clf.fit(X_train, classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(output,target) :\n",
    "    error = 0\n",
    "    for k in range(output.shape[0]) :\n",
    "        if abs(int(output[k])- int(target[k])) != 0 :\n",
    "            error += 1\n",
    "    return error/output.shape[0]\n",
    "\n",
    "def calculate_class_error(output,target):\n",
    "    error=6*[0]\n",
    "    pop=6*[0]\n",
    "    for k in range(output.shape[0]) :\n",
    "        pop[int(target[k])]+=1\n",
    "        if abs(int(output[k])- int(target[k])) != 0 :\n",
    "            error[int(target[k])] += 1\n",
    "    for i in range(6):\n",
    "        error[i]/=pop[i]\n",
    "        print(i,\" : \",error[i]*100)\n",
    "    \n",
    "    \n",
    "def noRT(out) :\n",
    "    if out == 0 :\n",
    "        return 0\n",
    "    return 3*10**(out-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_class_error(output,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction per class\n",
    "\n",
    "For each classes, we will build a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = int(np.amax(classes) + 1)\n",
    "classes_entr = clf.predict(X)\n",
    "# classes_entr = model_cl.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We regroup each class in a dictionnary according to the classification by the first model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_per_class(X,y,classes) :\n",
    "    dataset = {}\n",
    "    for k in range(X.shape[0]) :\n",
    "        f = int(classes[k])\n",
    "        # f = np.argmax(classes[k])\n",
    "        try : \n",
    "            dataset[f] +=1\n",
    "        except :\n",
    "            dataset[f] = 1\n",
    "    dataset2 = {}\n",
    "    for k in range(int(np.amax(classes)+1)) :\n",
    "    # for k in range(classes.shape[1]):\n",
    "        dataset2[str(k)] = {}\n",
    "        dataset2[str(k)][\"X\"] = np.zeros((dataset[k],X.shape[1]))\n",
    "        dataset2[str(k)][\"y\"] = np.zeros(dataset[k])\n",
    "    \n",
    "    indices = {\n",
    "        '0' : 0,\n",
    "        '1' : 0,\n",
    "        '2' : 0,\n",
    "        '3' : 0,\n",
    "        '4' : 0,\n",
    "        '5' : 0\n",
    "    }\n",
    "    for k in range(X.shape[0]) :\n",
    "        f = str(int(classes[k]))\n",
    "        # f = str(np.argmax(classes[k]))\n",
    "        indice = indices[f]\n",
    "        indices[f] +=1\n",
    "        dataset2[f][\"X\"][indice,:] = X[k,:]\n",
    "        dataset2[f][\"y\"][indice] = y[k]\n",
    "    return dataset2\n",
    "\n",
    "\n",
    "dataset_per_class = create_dataset_per_class(X,y,classes_entr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_class(dataset,class_to_get) :\n",
    "    return dataset[str(class_to_get)][\"X\"], dataset[str(class_to_get)][\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the models will be alike\n",
    "\n",
    "*d* contains the training set and the model associated with the class.\n",
    "We tried different learning model, such as neural network, random forrest and gradient tree boosting. The first gave us the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def train_models(dataset_per_class) :\n",
    "    for k in dataset_per_class :\n",
    "        if int(k) != 0 : # we do not train the first class as it is only a 0 prediction\n",
    "            X_test, y_test = get_dataset_class(dataset_per_class,int(k))\n",
    "            # neural network\n",
    "            print(X_test.shape,y_test.shape)\n",
    "            \n",
    "            Train,a,train,b = train_test_split(X_test,y_test,test_size = 0.5)\n",
    "            model = Sequential()\n",
    "            model.add(Dense(64, input_dim=21, activation=\"sigmoid\"))\n",
    "            model.add(Dense(128, activation=\"relu\"))\n",
    "            model.add(Dense(256, activation=\"relu\"))\n",
    "            model.add(Dense(32, activation=\"relu\"))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "            history = model.fit(Train, train,validation_data = (a,b), epochs=45, batch_size=64)\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            # random forrest\n",
    "            \"\"\"model = RandomForestRegressor(n_estimators=200,max_depth=30, random_state=0)\n",
    "            model.fit(Train,train)\"\"\"\n",
    "            \n",
    "            #Gradient tree bossting\n",
    "            \"\"\"model = GradientBoostingRegressor(n_estimators=400,max_depth=30,random_state=0)\n",
    "            model.fit(Train,train)\n",
    "            GradientBoostingRegressor\"\"\"\n",
    "            \n",
    "            dataset_per_class[str(int(k))][\"model\"] = model\n",
    "    return dataset_per_class\n",
    "\n",
    "d = train_models(dataset_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to compute these two steps on our evaluation data\n",
    "\n",
    "### Transform eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval,id_eval = from_pd_to_numpy(data_eval,training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_eval = clf.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class these data according to their class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_eval_classes(X_eval,id_eval,eval_classes) :\n",
    "    dataset = {}\n",
    "    for k in range(X_eval.shape[0]) :\n",
    "        try : \n",
    "            f = int(eval_classes[k]) # we have a prediction like 0 or 1 or 2\n",
    "        except :\n",
    "            f = np.argmax(eval_classes[k]) # we have something like [0.2,0.3,0.4,0.1]\n",
    "        try : \n",
    "            dataset[f] +=1\n",
    "        except :\n",
    "            dataset[f] = 1\n",
    "    dataset2 = {}\n",
    "    for k in range(int(np.amax(eval_classes)+1)) :\n",
    "    # for k in range(eval_classes.shape[1]) :\n",
    "        dataset2[str(k)] = {}\n",
    "        dataset2[str(k)][\"X\"] = np.zeros((dataset[k],X_eval.shape[1]))\n",
    "        dataset2[str(k)][\"id\"] = np.zeros(dataset[k])\n",
    "        dataset2[str(k)][\"class\"] = np.zeros(dataset[k])\n",
    "    \n",
    "    indices = {\n",
    "        '0' : 0,\n",
    "        '1' : 0,\n",
    "        '2' : 0,\n",
    "        '3' : 0,\n",
    "        '4' : 0,\n",
    "        '5' : 0\n",
    "    }\n",
    "    for k in range(X_eval.shape[0]) :\n",
    "        try :\n",
    "            f = str(int(eval_classes[k]))\n",
    "        except :\n",
    "            f = str(np.argmax(eval_classes[k])) # we have something like [0.2,0.3,0.4,0.1]\n",
    "        indice = indices[f]\n",
    "        indices[f] +=1\n",
    "        dataset2[f][\"X\"][indice,:] = X_eval[k,:]\n",
    "        dataset2[f][\"id\"][indice] = id_eval[k]\n",
    "        dataset2[f][\"class\"][indice] = int(f)\n",
    "    return dataset2\n",
    "\n",
    "dataset_eval = create_dataset_eval_classes(X_eval,id_eval,classes_eval) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the previous model on each class\n",
    "\n",
    "The prediction will be stored in *res*, which will then be ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(dataset_models,dataset_eval) :\n",
    "    res = {}\n",
    "    for indice in dataset_eval['0'][\"id\"] :\n",
    "        res[int(indice)] = 0\n",
    "    for k in dataset_eval :\n",
    "        if int(k) != 0 :\n",
    "            X_pred = dataset_eval[k][\"X\"]\n",
    "            prediction = dataset_models[k][\"model\"].predict(X_pred)\n",
    "            for i in range(len(dataset_eval[k][\"id\"])) :\n",
    "                res[int(dataset_eval[k][\"id\"][i])] = int(prediction[i])\n",
    "    return res\n",
    "    \n",
    "res = evaluate_models(d,dataset_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to order the results by the tweet ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "od = collections.OrderedDict(sorted(res.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"TweetID\"] = od.keys()\n",
    "df[\"NoRetweets\"] = od.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results/res21.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
