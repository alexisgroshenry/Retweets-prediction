{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 665777 tweets.\n",
      "evaluation : 285334 tweets.\n"
     ]
    }
   ],
   "source": [
    "#our data\n",
    "\n",
    "train=pd.read_csv(\"data/train.csv\")\n",
    "evaluation=pd.read_csv(\"data/evaluation.csv\")\n",
    "print(f'train : {len(train[\"text\"])} tweets.')\n",
    "print(f'evaluation : {len(evaluation[\"text\"])} tweets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000 tweets.\n"
     ]
    }
   ],
   "source": [
    "#data to train and evaluate the model\n",
    "#https://www.kaggle.com/kazanova/sentiment140 \n",
    "\n",
    "df = pd.read_csv('../sentiment_data_set/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\", header=None)\n",
    "df.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "print(f'{len(df[\"text\"])} tweets.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diminishing the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 tweets.\n"
     ]
    }
   ],
   "source": [
    "indices = random.sample(range(1600000),400000) #choose randomly 400000 tweets among the 1600000\n",
    "df = df.iloc[indices]\n",
    "print(f'{len(df[\"text\"])} tweets.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using TweetTokenizer to preprocess tweets\n",
    "\n",
    "# The reduce_len parameter will allow a maximum of 3 consecutive repeating characters, while trimming the rest\n",
    "# For example, it will tranform the word: 'Helloooooooooo' to: 'Hellooo'\n",
    "tk = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Separating our features (text) and our labels into two lists to smoothen our work\n",
    "X = df['text'].tolist()\n",
    "Y = df['label'].tolist()\n",
    "\n",
    "# Building our data list, that is a list of tuples, where each tuple is a pair of the tokenized text\n",
    "# and its corresponding label\n",
    "for x, y in zip(X, Y):\n",
    "    if y == 4:\n",
    "        data.append((tk.tokenize(x), 1))\n",
    "    else:\n",
    "        data.append((tk.tokenize(x), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'want', 'the', 'sun', '!', '!', 'but', \"I'm\", 'happy', 'today', '..'],\n",
       " 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/capucineleroux/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/capucineleroux/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        # First, we will convert the pos_tag output tags to a tag format that the WordNetLemmatizer can interpret\n",
    "        # In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb.\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'the', 'sun', '!', '!', 'but', \"I'm\", 'happy', 'today', '..']\n"
     ]
    }
   ],
   "source": [
    "# Previewing the WordNetLemmatizer() output\n",
    "print(lemmatize_sentence(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/capucineleroux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# Stopwords are frequently-used words (such as “the”, “a”, “an”, “in”) that do not hold any meaning useful to extract sentiment.\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "# A custom function defined in order to fine-tune the cleaning of the input text. This function is highly dependent on each usecase.\n",
    "# Note: Only include misspelling or abbreviations of commonly used words. Including many minimally present cases would negatively impact the performance. \n",
    "def cleaned(token):\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow' or token == '2moro':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token in ['hahah', 'hahaha', 'hahahaha']:\n",
    "        return 'haha'\n",
    "    if token == \"mother's\":\n",
    "        return \"mother\"\n",
    "    if token == \"mom's\":\n",
    "        return \"mom\"\n",
    "    if token == \"dad's\":\n",
    "        return \"dad\"\n",
    "    if token == 'bday' or token == 'b-day':\n",
    "        return 'birthday'\n",
    "    if token in [\"i'm\", \"don't\", \"can't\", \"couldn't\", \"aren't\", \"wouldn't\", \"isn't\", \"didn't\", \"hadn't\",\n",
    "                 \"doesn't\", \"won't\", \"haven't\", \"wasn't\", \"hasn't\", \"shouldn't\", \"ain't\", \"they've\"]:\n",
    "        return token.replace(\"'\", \"\")\n",
    "    if token in ['lmao', 'lolz', 'rofl']:\n",
    "        return 'lol'\n",
    "    if token == '<3':\n",
    "        return 'love'\n",
    "    if token == 'thanx' or token == 'thnx':\n",
    "        return 'thanks'\n",
    "    if token == 'goood':\n",
    "        return 'good'\n",
    "    if token in ['amp', 'quot', 'lt', 'gt', '½25', '..', '. .', '. . .']:\n",
    "        return ''\n",
    "    return token\n",
    "\n",
    "\n",
    "# This function will be our all-in-one noise removal function\n",
    "def remove_noise(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token in tweet_tokens:\n",
    "        # Eliminating the token if it is a link\n",
    "        token = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", token)\n",
    "        # Eliminating the token if it is a mention\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        \n",
    "        cleaned_token = cleaned(token.lower())\n",
    "        \n",
    "        if cleaned_token == \"idk\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('dont')\n",
    "            cleaned_tokens.append('know')\n",
    "            continue\n",
    "        if cleaned_token == \"i'll\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"you'll\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"we'll\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"it'll\":\n",
    "            cleaned_tokens.append('it')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"it's\":\n",
    "            cleaned_tokens.append('it')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"i've\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"you've\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"we've\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"they've\":\n",
    "            cleaned_tokens.append('they')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"you're\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"we're\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"they're\":\n",
    "            cleaned_tokens.append('they')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"let's\":\n",
    "            cleaned_tokens.append('let')\n",
    "            cleaned_tokens.append('us')\n",
    "            continue\n",
    "        if cleaned_token == \"she's\":\n",
    "            cleaned_tokens.append('she')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"he's\":\n",
    "            cleaned_tokens.append('he')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"that's\":\n",
    "            cleaned_tokens.append('that')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"i'd\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('would')\n",
    "            continue\n",
    "        if cleaned_token == \"you'd\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('would')\n",
    "            continue\n",
    "        if cleaned_token == \"there's\":\n",
    "            cleaned_tokens.append('there')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"what's\":\n",
    "            cleaned_tokens.append('what')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"how's\":\n",
    "            cleaned_tokens.append('how')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"who's\":\n",
    "            cleaned_tokens.append('who')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"y'all\" or cleaned_token == \"ya'll\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('all')\n",
    "            continue\n",
    "\n",
    "        if cleaned_token.strip() and cleaned_token not in string.punctuation: \n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "            \n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'want', 'the', 'sun', 'but', 'im', 'happy', 'today']\n"
     ]
    }
   ],
   "source": [
    "# Previewing the remove_noise() output\n",
    "print(remove_noise(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokens_list = []\n",
    "\n",
    "# Removing noise from all the data\n",
    "for tokens, label in data:\n",
    "    cleaned_tokens_list.append((remove_noise(tokens), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i', 'want', 'the', 'sun', 'but', 'im', 'happy', 'today'], 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing our final (tokenized, cleaned and lemmatized) data list\n",
    "cleaned_tokens_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a handy function in order to load a given glove file\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the 50-dimensional GloVe embeddings\n",
    "# This method will return three dictionaries:\n",
    "# * word_to_index: a dictionary mapping from words to their indices in the vocabulary\n",
    "# * index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary\n",
    "# * word_to_vec_map: dictionary mapping words to their GloVe vector representation\n",
    "# Note that there are 400,001 words, with the valid indices ranging from 0 to 400,000\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../sentiment_data_set/glove.6B/glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176468"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['hello'] #unk is the code for unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 means opposed, 0 means decorrelated, 1 means very correlated meaning\n",
    "def cosine_similarity(u, v):\n",
    "    dot = np.dot(u, v)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "unks = []\n",
    "UNKS = []\n",
    "\n",
    "# This function will act as a \"last resort\" in order to try and find the word\n",
    "# in the words embedding layer. It will basically eliminate contiguously occuring\n",
    "# instances of a similar character\n",
    "def cleared(word):\n",
    "    res = \"\"\n",
    "    prev = None\n",
    "    for char in word:\n",
    "        if char == prev: continue\n",
    "        prev = char\n",
    "        res += char\n",
    "    return res\n",
    "\n",
    "\n",
    "def train_sentence_to_indices(sentence_words, word_to_index, max_len, i):\n",
    "    global X, Y\n",
    "    sentence_indices = []\n",
    "    for j, w in enumerate(sentence_words):\n",
    "        try:\n",
    "            index = word_to_index[w]\n",
    "        except:\n",
    "            UNKS.append(w)\n",
    "            w = cleared(w)\n",
    "            try:\n",
    "                index = word_to_index[w]\n",
    "            except:\n",
    "                index = word_to_index['unk']\n",
    "                unks.append(w)\n",
    "        X[i][j] = index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    }
   ],
   "source": [
    "# Here we will utilize the already computed 'cleaned_tokens_list' variable\n",
    "\n",
    "list_len = [len(i) for i, j in cleaned_tokens_list]\n",
    "max_len = max(list_len)\n",
    "print(max_len)\n",
    "\n",
    "#X = []\n",
    "#for i in range (len(cleaned_tokens_list)) :\n",
    "#    row = []\n",
    "#    for j in range (max_len):\n",
    "#        row.append(0.)\n",
    "#    X.append(row)\n",
    "X = np.zeros((len(cleaned_tokens_list), max_len))\n",
    "Y = np.zeros((len(cleaned_tokens_list), ))\n",
    "for i, tk_lb in enumerate(cleaned_tokens_list):\n",
    "    tokens, label = tk_lb\n",
    "    train_sentence_to_indices(tokens, word_to_index, max_len, i)\n",
    "    Y[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185457. 383068. 357266. 347345.  87775. 187631. 173081. 361080.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.]\n",
      "1.0\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(Y[0])\n",
    "print(len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"X\"] = X\n",
    "df[\"Y\"] = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the prepocessing result if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"preprocessed_sentiment_data.csv\")\n",
    "#np.save(\"preprocessed_sentiment_data\",df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the saved results if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = np.load(\"preprocessed_sentiment_data.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(df,columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\",\"Y\",\"X\"])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df[\"X\"].values\n",
    "#Y = df[\"Y\"].values\n",
    "#max_len = len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 80000 tweets.\n",
      "test : 20000 tweets.\n"
     ]
    }
   ],
   "source": [
    "print(f\"train : {len(Y_train)} tweets.\")\n",
    "print(f\"test : {len(Y_test)} tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that will initialize and populate our embedding layer\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"unk\"].shape[0] #50\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False, input_shape=(max_len,))\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a sequencial model composed of firstly the embedding layer, than a pair of Bidirectional LSTMs,\n",
    "# that finally feed into a sigmoid layer that generates our desired output between 0 and 1.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\n",
    "#model.add(Dropout(rate=0.4))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "#model.add(Dropout(rate=0.4))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 229, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 229, 256)          183296    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 20,577,843\n",
      "Trainable params: 577,793\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling our model with a binary cross-entropy loss function, using the default adam optimizer\n",
    "# and setting the accurary as the metric to track and ameliorate\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/capucineleroux/opt/anaconda3/envs/data_challenge/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20480/80000 [======>.......................] - ETA: 47:31 - loss: 0.6198 - accuracy: 0.6522"
     ]
    }
   ],
   "source": [
    "# Setting a batch size of 20 and training our model for 20 epochs\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 20, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence_words, max_len):\n",
    "    X = np.zeros((max_len))\n",
    "    sentence_indices = []\n",
    "    for j, w in enumerate(sentence_words):\n",
    "        try:\n",
    "            index = word_to_index[w]\n",
    "        except:\n",
    "            w = cleared(w)\n",
    "            try:\n",
    "                index = word_to_index[w]\n",
    "            except:\n",
    "                index = word_to_index['unk']\n",
    "        X[j] = index\n",
    "    return X\n",
    "\n",
    "def predict_tweet_sentiment(custom_tweet):\n",
    "    # Convert the tweet such that it can be fed to the model\n",
    "    x_input = sentence_to_indices(remove_noise(tk.tokenize(custom_tweet)), max_len)\n",
    "    \n",
    "    # Return the model's prediction\n",
    "    return model.predict(np.array([x_input])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_tweets = len(train[\"text\"])\n",
    "nb_evaluation_tweets = len(evaluation[\"text\"])\n",
    "\n",
    "sentiment_train = []\n",
    "sentiment_evaluation = []\n",
    "\n",
    "for i in range (nb_train_tweets):\n",
    "    text = train[\"text\"][i]\n",
    "    sentiment_train.append(predict_tweet_sentiment(text))\n",
    "train[\"sentiment\"] = sentiment_train\n",
    "\n",
    "for i in range (nb_evaluation_tweets):\n",
    "    text = evaluation[\"text\"][i]\n",
    "    sentiment_evaluation.append(predict_tweet_sentiment(text))\n",
    "evaluation[\"sentiment\"] = sentiment_evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
